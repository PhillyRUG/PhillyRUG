{
  "hash": "c4b5bfe43d5fce21256bb7bebe2ddcff",
  "result": {
    "markdown": "---\ntitle: \"JC: Downside Risk Evaluation + GAS\"\nauthor: \"Cory Cutsail, PhillyRUG\"\neditor: visual\n---\n\n\nThe GAS package was written for Generalized Autoregressive Score models, which include GARCH as a special case. This paper hones in on how to use the package to estimate Value at Risk (VaR) and Expected Shortfall (ES). These are common financial metrics that can be relatively involved to compute. Let's dive in.\n\nFirst, a brief discussion on Value at Risk. An informal definition (per Wikipedia): given a portfolio, a time horizon, and a probability p, the VaR is the maximum possible loss over the time horizon excluding the tail to the left of p. More directly, given a profit and loss distribution $X$, the p-VaR is the $(1-p)$ quantile of $Y:=-X$  \n\nI find this a little unintuitive --- can we just define the p-VaR as the p-quantile of X? Likely not, but a question for the group.\n\nOk, how about Expected Shortfall? ES is a similar risk measure, defined at a given level q as the expected return on a portfolio in the worst q% of cases (again, from Wikipedia). This one is a bit funkier mathematically:\n\n\n$$ES_\\alpha (X) = -\\frac{1}{\\alpha}\\int\\limits_0^\\alpha VaR_\\gamma (X) d\\gamma$$\n\n\nIf our VaR distribution is approximately normal (it probably isn't), the ES might look something like this:\n\n![](images/pic.png)\n\nWhere the purple shaded region is our ES. \n\nThe plain-English definition is given as the mean loss of portfolio value, given a loss is occurring at or below a given quantile.\n\nDespite the fact that our VaR likely isn't normally distributed, the underlying portfolio value may be well-approximated by a normal distribution. My reading is that it is more common in practice to use a t-distribution as they have fatter tails (implying more realistic modeling of potential downside risk). Both the normal and t-distributions have closed form solutions for Expected Shortfall. The normal distribution with mean \\$\\\\mu\\$ and variance $\\sigma^2$ has $\\alpha$-ES\n\n\n$$\\mu + \\sigma\\frac{\\varphi(\\Phi^{-1}(\\alpha))}{1-\\alpha}$$\n\n\nWe see that ES is monotonically increasing in the variance and monotonically decreasing in the mean. \n\nThe dynamics in $\\alpha$ are a bit more interesting. Discussion point: what should they look like for this special case? Keep in mind that alpha is on the unit interval.\n\nNow that we've covered the fundamentals of what we'd like to estimate, how do we go about estimating it? Note that for both risk measures, we need a notion of the underlying density. We don't necessarily want to assume normality here: this is where Generalized Autoregressive Score models come into play. Here we'll use Siem Jan Koopman's AENORM article 'Generalized Autoregressive Score Models'. \n\nLet's start with the building blocks.\n\nWe have a dependent variable $y_t$ Nx1, a parameter vector $f_t$, and a vector of (exogenous) covariates $x_t$ . The available information at time t is given by $\\{f_t, \\mathcal{F}_t \\}$\n\nWhere $$\\mathcal{F}_t = \\{\\{ y_1,...,y_t\\},\\{f_0,...,f_{t-1}\\},\\{x_1,...,x_t\\}\\}$$\n\nand that $$y_t\\sim p(y_t|f_t\\mathcal{F}_t)$$\n\nWe also have a vector of static parameters $\\theta$\n\nOur time-t parameter is updated via \n\n\n$$ f_{t+1} = \\omega + \\sum\\limits_{i=1}^p A_i s_{t-i+1} + \\sum\\limits_{j=1}^q B_j f_{t-j+1}  $$\n\n\nThe omega here is a vector of constants, A and B are coefficient matrices, and s is some (appropriately selected) function of historical data. Koopman refers to this as the familiar autoregressive updating equation \\-- if it isn't familiar at this juncture, it might be when we show that GARCH models are a special case of GAS models. The omega and the coefficient matrices here are all functions of our static parameter vector theta.\n\nThe update equation is where the Score part of Generalized Autoregressive Score models come into play. We have an update equation for our \\$f_t\\$ given by $s_t = S_t \\cdot\\nabla_t$ with $$ \\nabla_t = \\frac{\\partial \\ln p(y_t | f_t,\\mathcal{F}_t; \\theta)}{\\partial f_t}  $$ , $$S_t = S(t,f_t,\\mathcal{F}_t;\\theta)$$ where S is a 'matrix function'. We get to pick it! It simply scales the score.\n\nIf we choose S to be $$S_t = \\mathcal{I}^{-1}_{t|t-1}, \\mathcal{I_{t|t-1}} = \\mathbb{E}[\\nabla_t\\nabla_t']$$ and are clever in the remainder of our model specification, we'll end up with the GARCH model. Aside: if I remember correctly, this S is the Fisher information matrix.\n\nOnto the GARCH model: assume $y_t=\\sigma_t\\varepsilon_t$ where epsilon is iid Gaussian with zero mean and unit variance. Koopman lays out as an exercise to show that the GAS(1,1) model reduces to\n\n\n$$f_{t+1} = \\omega + A_1(y_t^2-f_t) + B_1f_t = \\alpha_0 + \\alpha_1 y_t^2 + \\beta_1 f_t$$\n\n\nI don't have time for this exercise at the moment but it could yield interesting discussion. Note that this equation is the familiar GARCH(1,1), and that GAS(1,1) = GAS(p=1,q=1) where p and q are the order of the initial autoregressive update equation.\n\nOk, so we've covered the basics of GAS models, VaR, and ES. Let's get into the paper we want to talk about: Downside Risk Evaluation with the R Package GAS. We'll change our notation up a bit at this juncture to keep consistent with the paper. Namely, \n\n\n$$y_t\\sim p(y_t|f_t\\mathcal{F}_t) \\mapsto r_t | \\mathcal{I_{t-1}} \\sim \\mathcal{SKST}(r_t;\\mu,\\sigma_t,\\xi,\\nu) $$\n\n\nNote that the only time-varying parameter here is the scale/volatility parameter \\$\\\\sigma_t\\$. The location parameter, mu, and the skewness and shape parameters xi and nu are time-invariant.\n\nFrom here, the authors go on to derive the update equation and note that both a Gaussian and Student-t distribution are special cases of the SKST distribution with appprpriate assumptions on the skewness and shape parameters. We'll gloss over this bit for now.\n\nThe objective here is to backtest VaR and ES models. The authors describe two approaches to backtesting VaR:\n\n-   \"Unconditional Coverage tests look at the ratio between the number of realized VaR violations observed from the data and the expected number of VaR violations implied by the chosen risk level, α, during the forecast period\"\n\n-   Conditional Coverage tests examine the distribution of VaR exceedences/violations: if correct conditional coverage is attained, these should be independently distributed over time.\n\nThey then specify a DQ test that's ultimately a Wald test on the joint hypothesis that the 'hitting series' (VaR exceedences) has zero coefficients for all lags of appropriate order, as do the lagged coefficients on VaR in the same equation. This test is asymptotically $\\chi^2(L+2)$, and the original paper outlining the test suggest lag-order 4. I am guessing this depends on the underlying frequency of the data, but the authors note that this has become a standard assumption.\n\nThe authors go on to discuss model comparison - we'll skip this for now but it does have practical implications in terms of the ultimately selected model.\n\nWe'll also omit discussion of joint VaR and ES comparison as it depends on the quantile loss function established in the prior section. We'll hit on it when running their empirical analysis ... Now!\n\nWe'll follow along with the paper directly moving forward, starting by loading in data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(GAS)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'GAS'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:stats':\n\n    residuals\n```\n:::\n\n```{.r .cell-code}\nlibrary(parallel)\nlibrary(dplyr)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'dplyr'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n```\n:::\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibrary(tidyr)\ndata(\"dji30ret\", package = \"GAS\")\ndji30ret <- tail(dji30ret, 2500)\ndji30ret_plt <- dji30ret\ndji30ret_plt$dt <- row.names(dji30ret_plt)\ndji30ret_plt |> \n  pivot_longer(cols=-c(dt)) |> \n  mutate(dt=as.Date(dt)) |> \n  ggplot(aes(x=dt,y=value)) + geom_point(alpha=0.4) + \n  scale_x_date(breaks='1 year') + \n  theme(axis.text.x = element_text(angle = 45, vjust = 0.5, hjust=1))\n```\n\n::: {.cell-output-display}\n![](jc_gas_20221108_files/figure-html/data-load-1.png){width=672}\n:::\n:::\n\n\nNext, we'll specify univariate GAS models with underlying normal, student-T and skewed student-T distributions:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nGASSpec_N <- UniGASSpec(Dist = \"norm\", GASPar = list(scale = TRUE))\nGASSpec_ST <- UniGASSpec(Dist = \"std\", GASPar = list(scale = TRUE))\nGASSpec_SKST <- UniGASSpec(Dist = \"sstd\", GASPar = list(scale = TRUE))\ncluster <- makeCluster(2)\nH <- 1000\nRoll_N <- UniGASRoll(dji30ret[, \"GE\"], GASSpec_N, RefitEvery = 5,\ncluster = cluster, ForecastLength = H)\nalpha <- 0.01\nVaR_N <- quantile(Roll_N, probs = alpha)\nES_N <- ES(Roll_N, probs = alpha)\nVaRBacktest_N <- BacktestVaR(data = tail(dji30ret[, \"GE\"], H), VaR = VaR_N, alpha = alpha)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nFZL <- FZLoss(data = tail(dji30ret[, \"GE\"], H), VaR = VaR_N, ES = ES_N,\n                alpha = alpha)\n```\n:::\n",
    "supporting": [
      "jc_gas_20221108_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}